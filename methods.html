<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Protein Dynamics - Methods</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>


	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->

					<header id="header">
						<h1>Methods</h1>
					</header>


				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="index.html#summary">Home</a></li>
							<li><a href="intro.html">Introduction</a></li>
							<li><a href="methods.html" class="active">Methods</a></li>
							<li><a href="results.html">Results</a></li>
							<li><a href="discussion.html">Discussion and Conclusion</a></li>
							<li><a href="references.html">References</a></li>
						</ul>
					</nav>



				<!-- Main -->
					<div id="main">

							<!-- intro-->
							<section id="intro" class="main">
								<div class="content">

									<h3><b>Section Contents</b></h3>

									<ol style="text-align: left">
										<li><a href="#background"><b>Background</b></a></li>
										<li><a href="#model"><b>Learned dynamics model</b></a></li>
										<li><a href="#pbmp"><b>Physics-based message-passing</b></a></li>
										<li><a href="#dataset"><b>Dataset</b></a></li>
										<li><a href="#training"><b>Training</b></a></li>
										<li><a href="#assesment"><b>Model assesment</b></a></li>
									</ol>

									<h3 id="background"><b>Background</b></h3>

									<p>

									The following link to pages detailing background knowledge for those less farmilar with the computational and machine learning methods used here.</p>


									<ul class="actions special">
										<li><a href="background/gnns.html" class="button">Graph Neural Networks</a></li>
										<li><a href="background/keops.html" class="button">Symbolic Matrices</a></li>

									</ul>

										

										<h3 id="model"><b>Learnt dynamics model</b></h3>

										<figure id="dyanmics">
											<img src="images/dynamics.png" width=700/>
											<figcaption>Figure 2: An overview of our dynamics model d<sub>θ</sub> .</figcaption>           
										</figure><br>


										<p>We construct a DMS simulator for proteins (as seen in <a href="intro.html#dms">Figure 1</a>) where d<sub>θ</sub> is outlined in <a href="#dynamics">Figure 2</a>. For every N number of steps, the atoms enter d<sub>θ</sub> as a set of points in space. Edges are drawn between the atoms using a fast KNN algorithm implemented in KeOps such that distance matrices only need to be computed symbolically. Our physics-based message-passing procedure (see next section) employs 3 separate sub-networks to predict the potentials of atom-atom interactions based on distances, angles and dihedrals respectively. Forces are calculated by taking the gradient of the potentials at the given distance/angle. Finally, all forces acting on every atom are summed and final atom accelerations are computed.</p>

										<p>We use the same Verlet integrator as seen in <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span> as the Update function to compute the new velocities and coordinates. As the atoms are initialised with random starting velocities in every batch of training, the networks are forced to learn a dynamics model that can maintain the structure overtime (<a href="#training">Figure 3</a>). The simulator and the message-passing layers are implemented in <a href="https://pytorch.org/">Pytorch</a> (<span class="tooltip">Paszke et al. 2017<span class="tooltiptext">Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L. and Lerer, A., 2017. <a href="https://openreview.net/forum?id=BJJsrmfCZ">Automatic differentiation in pytorch</a>.</span></span>), making the whole function end-to-end differentiable.</p>

										<figure id="training">
											<img src="images/training.gif" width=800/>
											<figcaption>Figure 3: Progression towards learnt dynamics during training. Initially, all the potentials are flat and the model does not know how to stabalise the protein, leading the to atoms coming apart. As training progresses, the model learns to stabalise the atoms in the proteins and the RMSD loss is reduced. The potentials demonstrate graphically what is learnt by the neural networks.</figcaption>           
										</figure><br>

										<h3 id="pbmp"><b>Physics-based message-passing</b></h3>

										<p>We propose the use of a new GNN architecture, called physics-based message-passing (PBMP), which leverages our knowledge of the mechanics underlying molecular dynamics (<span class="tooltip">Monasse & Boussinot 2014<span class="tooltiptext">Monasse, B. & Boussinot, F. (2014), <i>‘Determination of forces from a potential in molecular dynamics’</i>, arXiv preprint <a href="https://arxiv.org/abs/1401.1181">arXiv:1401.1181</a>.</span></span>) and the inductive biases of GNNs (<span class="tooltip">Battaglia et al. 2018<span class="tooltiptext">Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R. et al. (2018), <i>‘Relational inductive biases, deep learning, and graph networks’</i>, arXiv preprint <a href="https://arxiv.org/abs/1806.01261">arXiv:1806.01261</a> .</span></span>, <span class="tooltip">Monti et al. 2017<span class="tooltiptext">Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J. & Bronstein, M. M. (2017), <i>Geometric deep learning on graphs and manifolds using mixture model cnns</i>, in ‘<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Monti_Geometric_Deep_Learning_CVPR_2017_paper.html">Proceedings of the IEEE conference on computer vision and pattern recognition</a>’, pp. 5115–5124.</span></span>). These physics-based equations could certainly be learnt by a neural network, but by integrating them into our message-passing procedure, we force the network to learn around these equations which makes our model interpretable. We define a basic PBMP as the following 4 equations:</p>


										\begin{equation}
										m_{ij} = \phi_e(h_i,h_j,e_{ij})
										\end{equation}
										\begin{equation*}
										\overrightarrow{F_{ij}} = \psi_p(m_{ij})
										\end{equation*}
										\begin{equation}
										\overrightarrow{F_i} = \sum_{j\in\mathcal{N}(i)}^{}\overrightarrow{F_{ij}}
										\end{equation}
										\begin{equation}
										\overrightarrow{a_i} = \overrightarrow{F_i}/mass_i
										\end{equation}

										
										<p>where <i>h</i> is the one-hot embeddings of the atom types, <i>e<sub>ij</sub></i> are the edge properties (e.g. Euclidean
										distance or bond angle), <i>φ<sub>e</sub></i> is a deep Multi Layer Perceptron (MLP) that
										constructs the one-dimensional messages <i>mij</i>, <i>ψp</i> is an equation derived from our knowledge of the
										physics of the systems. <i>ψ<sub>p</sub></i> in turn calculates the individual force vector (<i>F<sub>ij</sub></i>) that atom <i>j</i> exerts on
										atom <i>i</i>. <i>F<sub>i</sub></i> is the total force experienced by atom <i>i</i> and is the sum of all the forces exerted on it by the
										atoms in its neighbourhood <i><span>&#413;</span>(i)</i>. The final accelerations experienced by the atoms, <i>a<sub>i</sub></i>, are calculated using Newton’s 2nd Law of Motion. The masses of the individual atoms, <i>mass<sub>i</sub></i>, are the only physical or chemical properties given to the model. Whilst the masses of the atoms could definitely be learnt in an end-to-end fashion, adding Eq 4 gives us interpretability when forces act on atoms of different masses. While PBMP shares multiple properties with conventional message-passing on graphs, our method does not apply an MLP over the node attributes (as this is the experienced force) and we add physics-based equations. We also only employ one layer of PBMP per time step.</p>

										<p>In the distance submodule, <i>e<sub>ij</sub></i> also includes the absolute sequence seperation of the 2 atoms underconsideration. This allows for the model to learn differences between covalent bonds as well as local and global preferences. Eqs 1-4 describe the architecture of the distance sub-module shown in Figure 3. In the bond and dihedral angles sub-modules, we modify Equation 1 to take in 3 or 4 atom embeddings respectively. Thus, the ’messages’ constructed between multiple nodes/atoms encode the potential of that angle system. <i>ψ<sub>p</sub></i> in the angle and dihedral sub-modules can calculate the forces experienced by each atom correspondingly.</p>
										
										<p>Full derivation of the equations used in <i>ψ<sub>p</sub></i> are provided in <span class="tooltip">Monasse & Boussinot (2014)<span class="tooltiptext">Monasse, B. & Boussinot, F. (2014), <i>‘Determination of forces from a potential in molecular dynamics’</i>, arXiv preprint <a href="https://arxiv.org/abs/1401.1181">arXiv:1401.1181</a> .</span></span>. However, in brief, the messages can be thought of as the scalar value describing the magnitude of the force between multiple atoms (be they bonded or in an angle/dihedral) and ψp recovers this into a 3D force vector to be applied to each atom. For example, in the case of distance forces, this is simply achieved by multiplying <i>m<sub>ij</sub></i> by the normalised vector difference between atoms <i>i</i> and <i>j</i>  (<i>norm(ij)</i>).</p>

										<h3 id="dataset"><b>Dataset</b></h3>

										<p>
										<figure id="cg">
											<span class="image right"><img src="images/cg.jpg" width=500 alt=""/>
											<figcaption>Figure 4: Two representations of a protein structure graph. Left: an all-atom representation of a protein α-helix. Right: a course-grained representation of a protein as used here.<br></figcaption>      </span>      
										</figure>
										<br>

										In this work we use the same dataset from <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span> for training and evaluation. This dataset contains 2204 small protein structures (2004/200 train/test split) with a course-grained presentation (each residue has 4 atoms, C<span>&#x3B1</span>, C, N and side chain centroid) as seen in <a href="#cg">Figure 4</a>. We use this datasets for fair comparisons, the quality of the structures (better than 2.5 <span>&#8491;</span> resolutions and no missing internal residues) and small chain lengths (ideal for memory restrictions).</p>

										<h3 id="training"><b>Training</b></h3>

										<p>All training was done on a <a href="https://www.nvidia.com/en-gb/data-center/tesla-v100/">NVIDIA Tesla V100</a> and took 32 GB of GPU memory for one forward pass using a batch size of 1. A starting temperature of 0.05, time step of 0.05, k of 20 and learning rate of 0.0005 were used over 1 epoch where each sample was simulated for 180 time steps during training. No thermostat was used during training.</p>


										<h3 id="assesment"><b>Model assessment</b></h3>

										<p>Once we have trained the models, we can interpret the learnt dynamics by presenting the sub-networks with synthetic examples (e.g. 2 atoms at varying distances apart). These can then be used to graph potentials and compared to other force-fields (e.g. that in <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span> trained on the same data) and known statistics from chemical knowledge and the PDB (e.g. Mean Potential Force (<span class="tooltip">Zhou & Zhou 2002<span class="tooltiptext">Zhou, H. & Zhou, Y. (2002), <i>‘Distance-scaled, finite ideal-gas reference state improves structure-derived potentials of mean force for structure selection and stability prediction’</i>, Protein science 11(11), 2714– 2726, <a href="https://doi.org/10.1110/ps.0217002">doi.org/10.1110/ps.0217002</a>.</span></span>). This is only possible because we have integrated our knowledge of molecular dynamics into our neural network architecture.</p>

										<p>After training, sample trajectories were generated over 10,000 steps (structure saved every 100 steps) with a time step of 0.02 and temperature of 0.02. We use lower temperature factors during testing as this is less likely to cause instability (i.e. an atom to move out of it's energy well). Folding of small proteins over 10,000s of steps was also attempted. Animations were made in PyMol (<span class="tooltip">DeLano et al. 2002<span class="tooltiptext">DeLano, W. L. et al. (2002), <i>‘Pymol: An open-source molecular graphics tool’</i>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.231.5879&rep=rep1&type=pdf#page=44">CCP4 Newsletter on protein crystallography</a> 40(1), 82–92.</span></span>) and plots were produced using Matplotlib (<span class="tooltip">Hunter 2007<span class="tooltiptext">Hunter, J. D. (2007), <i>‘Matplotlib: A 2d graphics environment’</i>, IEEE Annals of the History of Computing 9(03), 90–95, <a href="https://doi.ieeecomputersociety.org/10.1109/MCSE.2007.55">doi.ieeecomputersociety.org/10.1109/MCSE.2007.55</a><</span></span>).</p>


										<footer class="major">
			
											<ul class="actions special">
												<li><a href="intro.html" class="button">Introduction</a></li>
												<li><a href="results.html" class="button">Results</a></li>
											</ul>
		
										</footer>
									</div>
								</div>
							</section>
					</div>

				<!-- Footer -->
				<footer class="major">


					<p class="copyright">&copy; Charlie Harris. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>

				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>