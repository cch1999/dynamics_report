<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Protein Dynamics - Methods</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->

					<header id="header">
						<h1>Discussion and Conclusion</h1>
					</header>


				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="index.html#summary">Home</a></li>
							<li><a href="intro.html">Introduction</a></li>
							<li><a href="methods.html">Methods</a></li>
							<li><a href="results.html">Results</a></li>
							<li><a href="discussion.html" class="active">Discussion and Conclusion</a></li>
							<li><a href="references.html">References</a></li>
						</ul>
					</nav>



				<!-- Main -->
					<div id="main">
							
							<section id="discussion" class="main">
									<div class="content">


										<h3><b>Section Contents</b></h3>

										<ol style="text-align: left">
											<li><a href="#potentials"><b>Discussion</b></a></li>
											<ul type="a" style="text-align: left">
												<li><a href="#potentials"><b>Limitations</b></a></li>
												<li><a href="#dynamics"><b>Future work</b></a></li>
											</ul>
											<li><a href="#dynamics"><b>Conclusion</b></a></li>
										</ol>
										<header class="major">
											<h2>Discussion</h2>
										</header>


										<p>A general observation is that the potentials learnt for simple distances and angles have minima in the right places but tend to be quite ”broad” in comparison to the force-field learnt in <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span>. This would mean that our model may allow too much conformational flexibility compared to the native dynamics of proteins, which will cause steric clashing and be a problem for many tasks (e.g. assessing the ’drugability’ of a binding pocket (<span class="tooltip">Loving et al. 2014<span class="tooltiptext">Loving, K.A., Lin, A. and Cheng, A.C., 2014. Structure-based druggability assessment of the mammalian structural proteome with inclusion of light protein flexibility. PLoS Comput Biol, 10(7), p.e1003741, <a href="https://doi.org/10.1371/journal.pcbi.1003741">doi.org/10.1371/journal.pcbi.1003741</a></span></span>). However, it is notable that the slopes of all our learned potentials are much smoother, due to the fact we are using universal function approximators instead of refining individual bins, which is much more susceptible to noise.</p>

										<figure id="future">
											<span class="image right"><img src="images/future_apporach.png" width=300 alt="" style="border-radius: 5%"/>
											<figcaption>Figure 10: Left: our current approach draws edges between all atoms based on the KNNs equally. Right: we propose using simple rules that draws edges between the nodes that are known to have covalent interactions (red), leaving the remaining non-covalent edges (black) to be constructed using either a KNN or connectivity radius approach.<br></figcaption></span>      
										</figure>

										<p>More importantly, the sidechain-sidechain interactions that stabilise the secondary and tertiary structure (<span class="tooltip">Kathuria et al. 2015<span class="tooltiptext">Kathuria, S.V., Chan, Y.H., Nobrega, R.P., Özen, A. and Matthews, C.R., 2016. Clusters of isoleucine, leucine, and valine side chains define cores of stability in high‐energy states of globular proteins: Sequence determinants of structure and stability. Protein Science, 25(3), pp.662-675, <a href="https://doi.org/10.1002/pro.2860">doi.org/10.1002/pro.2860</a>.</span></span>) were essentially unlearnt, even though MLPs should be able to approximate them in theory. This could be due to a number of factors; high learning rate, lack of exposure to tertiary flexibility due to insufficient number of time steps during training or a lack of signal-to-noise when compared to covalent interactions when minimising RMSD. We propose the latter issue should be resolved by creating 2 separate distance sub-networks, one for bonded and non-bonded interactions (<a href="#future">Figure 10</a>).</p>

										<p>Our method also does not take into account any explicit solvent, which is problematic as protein folding and dynamics is largely facilitated by the hydrophobic effect (<span class="tooltip">Bellissent-Funel et al. 2016<span class="tooltiptext">Bellissent-Funel, M.-C., Hassanali, A., Havenith, M., Henchman, R., Pohl, P., Sterpone, F., Van Der Spoel, D., Xu, Y. & Garcia, A. E. (2016), <i>‘Water determines the structure and dynamics of proteins’</i>, Chemical reviews 116(13), 7673–7697, <a href="https://dx.doi.org/10.1021%2Facs.chemrev.5b00664">dx.doi.org/10.1021%2Facs.chemrev.5b00664</a>.</span></span>, <span class="tooltip">Cheung et al. 2002<span class="tooltiptext">Cheung, M. S., Garc ́ıa, A. E. & Onuchic, J. N. (2002), <i>‘Protein folding mediated by solvation: water expulsion and formation of the hydrophobic core occur after the structural collapse’</i>, Proceedings of the National Academy of Sciences 99(2), 685–690, <a href="https://doi.org/10.1073/pnas.022387699">doi.org/10.1073/pnas.022387699</a>.</span></span>). This could be accounted for in the model by concatenating the solvent accessible area of every atom to its embeddings. Alternatively, simply concatenating the number of neighbours that every atom has would be a more efficient alternative.</p>

										<p>In general, the RMSF analysis (<a href="results.html#rmsf">Figure 9</a>) showed that our method models considerably more flexibility in most parts of the two proteins we analysed. This is not surprising given the broad potentials seen in <a href="results.html#potentialss">Figure 7</a>. The BBA results would seem to suggest that the beta-strands tend to be modeled with more acccuate flexibility than the alpha helical regions, with the second beta-sheet conforming to flexibility near that seen in the NMR ensemble. However, further analysis will neeed to be performed before any broad conclusions can be made. For example, the reduced RMSF seen in the second beta-sheet of BBA couple be due to the global alignment algorithums preference to align the structures along the beta-sheets as the alpha-helix detaches from the fold during simulation.</p>
											

										<p>The ultimate benchmark for our dynamics model will eventually be whether it can learn to fold small proteins, as was demonstrated with the learnt force-field in <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span>. Our dataset only contains small proteins (≤ 100 residues) too, meaning that the nature of inter-domain interactions will almost certainly not be learnt by any method trained on this dataset. However, the memory issues seen in our method, and protein DMS more generally, will need to be addressed before larger proteins can be considered for use in training. The KeOps library has also resulted in substantial memory improvements in other Geometric Deep Learning based apporaches with proteins (<span class="tooltip">Sverrisson et al. 2020<span class="tooltiptext">Sverrisson, F., Feydy, J., Correia, B. & Bronstein, M. (2020), <i>‘Fast end-to-end learning on protein surfaces’</i>, bioRxiv, <a href="https://doi.org/10.1101/2020.12.28.424589">doi.org/10.1101/2020.12.28.424589</a>.</span></span>) and future work could look into weather this could applied to other aspects of the DMS process (e.g. velocity integrators and energy calculations).</p>


										<p>As was observed in <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span>, like deep learning, DMS is sensitive to hyperparameters. Our method has the additional complication of the neural network hyperparameters which also need to be refined to balance expresitivity against memory requirements (as well as finding an optimum value of K) and is on going at the time of writing this report. Residual connections were added between the layers in the MLPs but this did not substantially increase performance.</p>

										<p>By far the biggest limitation of our approach is the high memory costs incurred at each time step due to our use of neural networks to predict the potentials. For every atom interaction, angle and dihedral, a whole MLP worth of activations needs to be stored for AD, in stark contrast to <span class="tooltip">Greener & Jones (2021)
											<span class="tooltiptext">
												Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
												<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
													doi:10.1101/2021.02.05.429941</b></a></span></span> which only activates the values in two adjacent energy bins to calculate gradients. While it should be noted that we simulated our proteins over considerably fewer timesteps than in <span class="tooltip">Greener & Jones (2021)
														<span class="tooltiptext">
															Greener, J. G. & Jones, D. T. (2021),<br> <i>‘Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins’</i>,<br> bioRxiv <br>
															<a href="https://doi.org/10.1101/2021.02.05.429941"><b>
																doi:10.1101/2021.02.05.429941</b></a></span></span>, so the comparison is only fair at short timescales, the results are still impressive given the substantially faster training time (~100x) and more efficent use of training data (1 epoch down from 45). This is a testimate to the generalising and learning power of modern deep learning that suggests these methods are promising given more training time and hyperparameter refinement.</p>

										<header>
											<h2><b>Future work</b></h2>
										</header>

										Recent work has shown that complex physics can be learnt by stacking many layers of message passing GNNs (<span class="tooltip">Sanchez-Gonzalez et al. 2020<span class="tooltiptext">Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J. & Battaglia, P. (2020), <i>Learning to simulate complex physics with graph networks</i>, in ‘<a href="https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html">International Conference on Machine Learning</a>’, PMLR, pp. 8459–8468.</span></span>). Whilst we have intentionally restricted ourselves to one layer of PBMP in this work, we ultimately think moving towards deeper architectures with multiple layers will be needed to gain enough expressive power. However, much thought would need to go into how this could be done whilst maintaining intrepretability, as it will be hard to unpack the individual contributions from each atom in the protein to the overall dynamics. In the following sections, we outline two recent areas of work that we think will be of substantial use to protein dynamics.
										<br>
										<header>
											<br><h3><b>Equivariant Message Passing Networks</b></h3>
										</header>

										<figure id="equi">
											<span class="image left">
												<video width="400" autoplay loop>
													<source src="images/equi.mp4" type="video/mp4">
													<source src="movie.ogg" type="video/ogg">
												  Your browser does not support the video tag.
												</video> 
											<figcaption>Figure 11: Examples of translation invariant and equivariant tasks. In translation invariance, the predicted output (e.g. the name or function of the whole protein/object) should not change as the input is translated in space. In a translation equivariant task (e.g. protein domain or image segmentation, if a tranformation is applied in the input domain (e.g. the input protein is shifted) the model should be able to perform the same transformation in the output domain.<br></figcaption>      </span>      
										</figure>



										<p>Recent work in the literature has addressed many problems with using GNNs on graphs where the nodes have spatial coordinates by endowing models with equivariance in the Euclidean group E(3) (<span class="tooltip">Satorras, Hoogeboom & Welling 2021<span class="tooltiptext">Satorras, V. G., Hoogeboom, E. & Welling, M. (2021), <i>‘E(n) equivariant graph neural networks’</i>, arXiv preprint <a href="https://arxiv.org/abs/2102.09844">arXiv:2102.09844</a>.</span></span>). The E(3) group describes the sets of transformations. An example of translation invariance is provided in <a href="#equi">Figure 11</a>. This is desirable for molecules as we would like these coordinates to be transformed in the same way that they would be transformed in real space. These methods have proved very effective in small molecule generation (<span class="tooltip">Satorras, Hoogeboom, Fuchs, Posner & Welling 2021<span class="tooltiptext">Satorras, V. G., Hoogeboom, E., Fuchs, F. B., Posner, I. & Welling, M. (2021), <i>‘E(n) equivariant normalizing flows for molecule generation in 3d’</i>, arXiv preprint <a href="https://arxiv.org/abs/2105.09016">arXiv:2105.09016</a>.</span></span>) and it is speculated that AlphaFold2 relies on an iterative equivariant architecture (<span class="tooltip">Fuchs et al. 2021<span class="tooltiptext">Fuchs, F. B., Wagstaff, E., Dauparas, J. & Posner, I. (2021), <i>‘Iterative se (3)-transformers’</i>, arXiv preprint <a href="https://arxiv.org/abs/2102.13419">arXiv:2102.13419</a>.</span></span>).</p>
										<p>However, we argue that E(3) equivariance is not suitable for protein structures as it does not preserve the handedness (chirality) of their structures. Instead, we propose the use of the special Euclidean group SE(3) as these are equivariant to rotations and translations but not reflections. These architectures currently exist (<span class="tooltip">Fuchs et al. 2020<span class="tooltiptext">Fuchs, F. B., Worrall, D. E., Fischer, V. & Welling, M. (2020), <i>‘Se (3)-transformers: 3d roto-translation equivariant attention networks’</i>, arXiv preprint <a href="https://arxiv.org/abs/2006.10503">arXiv:2006.10503</a> .</span></span>) but are more computationally expensive than their E(n) counter-parts (<span class="tooltip">Satorras, Hoogeboom & Welling 2021<span class="tooltiptext">Satorras, V. G., Hoogeboom, E. & Welling, M. (2021), <i>‘E(n) equivariant graph neural networks’</i>, arXiv preprint <a href="https://arxiv.org/abs/2102.09844">arXiv:2102.09844</a>.</span></span>).</p>

										<header>
											<h3><b>Message Passing Simplicial networks</b></h3>
										</header>

										<p>Another recent addition to the literature is Message Passing Simplicial Networks (<span class="tooltip">Bodnar et al. 2021<span class="tooltiptext">Bodnar, C., Frasca, F., Wang, Y. G., Otter, N., Montu ́far, G., Lio, P. & Bronstein, M. (2021), <i>‘Weisfeiler and lehman go topological: Message passing simplicial networks’</i>, arXiv preprint <a href="https://arxiv.org/abs/2103.03212">arXiv:2103.03212</a>.</span></span>), which allows message passing on Simplicial Complexes (SCs). SCs are topological objects which generalise graphs to higher dimensions. This allows them to capture the higher-order interactions present in complex systems (e.g. molecules and biological networks). The authors also showed that MPSN had greater expressive power compared to baseline GNNs when learning regular graph and can be equipped with orientation equivariant layers. We propose that MPSN could be used to model the multi-level interactions (atom, residue, motif and domain level) we see within proteins. For example, if the potential between 2 atoms is known, and a third atom is introduced, it will distort the electron cloud of the other atoms, thus changing the potential (<span class="tooltip">Goel et al. 2020<span class="tooltiptext">Goel, H., Yu, W., Ustach, V. D., Aytenfisu, A. H., Sun, D. & MacKerell, A. D. (2020), <i>‘Impact of elec- tronic polarizability on protein-functional group interactions’</i>, Physical Chemistry Chemical Physics 22(13), 6848–6860, <a href="https://doi.org/10.1039/D0CP00088D">doi.org/10.1039/D0CP00088D</a>.</span></span>). To the best of our knowledge, these effects have yet to be taken into account by any machine learning method to date.</p>

										<p>Initial analysis into the feasibility of this approach indicated there were a prohibitive number of 2-simplices (triangles) in the course-grained protein graph at biologically relevant connectivity thresholds (∼ 8 &#x212B;). Further work could try to alleviate these issues by constructing a policy that only draws 2-simplices between 3 atoms types that are deemed biologically relevant (e.g. only between sidechain centroids within a certain distance or involved in a catalytic triad (<span class="tooltip">Carter & Wells 1988<span class="tooltiptext">Carter, P. & Wells, J. A. (1988), <i>‘Dissecting the catalytic triad of a serine protease’</i>, Nature 332(6164), 564–568, <a href="https://doi.org/10.1038/332564a0">doi.org/10.1038/332564a0</a>.</span></span>).</p>

									</div>
							</section>


							<section id="assessment" class="main">
									<div class="content">
										<header class="major">
											<h2>Conclusion</h2>
										</header>
										<p>We have validated that here is potential to use deep learning approaches to learn the dynamics of free proteins using only static structures as training data. Whilst we have made memory reductions by the use of the KeOps library to compute distance matrices, we have ultimately not succeeded in our aim of reducing the memory requirements needed for protein DMS. This will need to be a major focus of future work.</p>

										<p>The eventual goal of DMS is not to simply relearn the parameters of a force-field. If our methods eventually reduce the need for large amounts of training data, we can move to study the dynamics of other protein interactions (notably protein-protein interactions, protein-drug interactions and protein aggregation) by using similar loss functions. It would also be worthwhile to investigate whether these loss functions are limited to graph structured data or could instead be applied to protein surface meshes (<span class="tooltip">Gainza et al. 2020<span class="tooltiptext">Gainza, P., Sverrisson, F., Monti, F., Rodola, E., Boscaini, D., Bronstein, M. & Correia, B. (2020), <i>‘Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning’</i>, Nature Methods 17(2), 184–192, <a href="https://doi.org/10.1038/s41592-019-0666-6">doi.org/10.1038/s41592-019-0666-6</a>.</span></span>) and point clouds (<span class="tooltip">Sverrisson et al. 2020<span class="tooltiptext">Sverrisson, F., Feydy, J., Correia, B. & Bronstein, M. (2020), <i>‘Fast end-to-end learning on protein surfaces’</i>, bioRxiv, <a href="https://doi.org/10.1101/2020.12.28.424589">doi.org/10.1101/2020.12.28.424589</a>.</span></span>).</p>

										<p>Before we can move onto all-atom representations, course-grained graph representations of proteins are likely to be of substantial use in future research for understanding protein dynamics and numerous other problems in structural biology. The author intends to add this as a granularity feature in a new version of the Graphein (<span class="tooltip">Jamasb et al. 2020<span class="tooltiptext">Jamasb, A. R., Li ́o, P. & Blundell, T. (2020), ‘Graphein-a python library for geometric deep learning and network analysis on protein structures’, bioRxiv, <a href="https://doi.org/10.1101/2020.07.15.204701">doi.org/10.1101/2020.07.15.204701</a>.</span></span>) package currently under development.</p>

										<p>Given the healthy scepticism of many scientists and clinicians about the use of machine learning in medicine, integrating explainability into our methods of understanding protein dynamics will be of vital importance if these methods are to be used to design new drugs and treatments in healthcare.</p>

										<footer class="major">
											<ul class="actions special">
												<li><a href="results.html" class="button">Results</a></li>
												<li><a href="references.html" class="button">References</a></li>
											</ul>
										</footer>
									</div>
							</section>
					</div>

				<!-- Footer -->
				<footer class="major">

					<p class="copyright">&copy; Charlie Harris. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>

				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>